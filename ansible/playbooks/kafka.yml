---
- name: Install and configure Kafka KRaft + Redis + Consumer
  hosts: kafka
  become: yes
  vars:
    kafka_version: "3.6.1"
    kafka_scala_version: "2.13"
    kafka_home: "/opt/kafka"
    kafka_user: "kafka"
    redis_max_memory: "100mb"

  tasks:
    # ========================================
    # PREREQUISITES
    # ========================================

    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install required packages
      apt:
        name:
          - openjdk-11-jdk
          - python3
          - python3-pip
          - redis-server
          - git
          - acl
        state: present

    - name: Install Python packages
      pip:
        name:
          - kafka-python
          - boto3
          - redis
        state: present

    # ========================================
    # KAFKA USER
    # ========================================

    - name: Create Kafka user
      user:
        name: "{{ kafka_user }}"
        system: yes
        shell: /bin/bash
        home: "/home/{{ kafka_user }}"
        create_home: yes

    # ========================================
    # KAFKA INSTALLATION
    # ========================================

    - name: Download Kafka
      get_url:
        url: "https://archive.apache.org/dist/kafka/{{ kafka_version }}/kafka_{{ kafka_scala_version }}-{{ kafka_version }}.tgz"
        dest: "/tmp/kafka.tgz"
        mode: "0644"

    - name: Extract Kafka
      unarchive:
        src: "/tmp/kafka.tgz"
        dest: "/opt/"
        remote_src: yes
        owner: "{{ kafka_user }}"
        group: "{{ kafka_user }}"
        creates: "/opt/kafka_{{ kafka_scala_version }}-{{ kafka_version }}"

    - name: Remove existing /opt/kafka directory if it exists (fix collision)
      file:
        path: "{{ kafka_home }}"
        state: absent

    - name: Create symlink for Kafka
      file:
        src: "/opt/kafka_{{ kafka_scala_version }}-{{ kafka_version }}"
        dest: "{{ kafka_home }}"
        state: link
        force: yes

    - name: Create Kafka data directory
      file:
        path: "{{ kafka_home }}/kraft-data"
        state: directory
        owner: "{{ kafka_user }}"
        group: "{{ kafka_user }}"
        mode: "0755"

    # ========================================
    # KAFKA KRAFT CONFIGURATION
    # ========================================

    - name: Generate Kafka cluster ID
      shell: "{{ kafka_home }}/bin/kafka-storage.sh random-uuid"
      register: cluster_id
      changed_when: false

    - name: Configure Kafka KRaft server.properties
      copy:
        dest: "{{ kafka_home }}/config/kraft/server.properties"
        content: |
          # KRaft mode (combined controller+broker)
          process.roles=broker,controller
          node.id=1
          controller.quorum.voters=1@localhost:9093

          # Listeners
          listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://localhost:9093
          advertised.listeners=PLAINTEXT://{{ ansible_default_ipv4.address }}:9092
          controller.listener.names=CONTROLLER
          listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT

          # Data directories
          log.dirs={{ kafka_home }}/kraft-data

          # Log retention (7 days)
          log.retention.hours=168
          log.segment.bytes=536870912
          log.retention.check.interval.ms=300000

          # Compression
          compression.type=lz4

          # Performance
          num.network.threads=3
          num.io.threads=8
          socket.send.buffer.bytes=102400
          socket.receive.buffer.bytes=102400
          socket.request.max.bytes=104857600
        owner: "{{ kafka_user }}"
        group: "{{ kafka_user }}"
        mode: "0644"

    - name: Format Kafka storage
      shell: "{{ kafka_home }}/bin/kafka-storage.sh format -t {{ cluster_id.stdout }} -c {{ kafka_home }}/config/kraft/server.properties"
      args:
        creates: "{{ kafka_home }}/kraft-data/meta.properties"
      become_user: "{{ kafka_user }}"

    # ========================================
    # KAFKA SYSTEMD SERVICE
    # ========================================

    - name: Create Kafka systemd service
      copy:
        dest: /etc/systemd/system/kafka.service
        content: |
          [Unit]
          Description=Apache Kafka (KRaft mode)
          After=network.target

          [Service]
          Type=simple
          User={{ kafka_user }}
          ExecStart={{ kafka_home }}/bin/kafka-server-start.sh {{ kafka_home }}/config/kraft/server.properties
          ExecStop={{ kafka_home }}/bin/kafka-server-stop.sh
          Restart=on-failure
          RestartSec=10

          [Install]
          WantedBy=multi-user.target
        mode: "0644"

    # ========================================
    # REDIS CONFIGURATION
    # ========================================

    - name: Configure Redis
      lineinfile:
        path: /etc/redis/redis.conf
        regexp: "{{ item.regexp }}"
        line: "{{ item.line }}"
      loop:
        - { regexp: "^bind", line: "bind 0.0.0.0" }
        - { regexp: "^maxmemory-policy", line: "maxmemory-policy allkeys-lru" }
        - { regexp: "^maxmemory", line: "maxmemory {{ redis_max_memory }}" }
      notify: restart redis

    # ========================================
    # KAFKA CONSUMER (kafka_to_s3.py)
    # ========================================

    - name: Create consumer directory
      file:
        path: /opt/consumer
        state: directory
        owner: "{{ kafka_user }}"
        group: "{{ kafka_user }}"
        mode: "0755"

    - name: Copy consumer script
      copy:
        src: "{{ playbook_dir }}/../../src/consumers/kafka_to_s3.py"
        dest: /opt/consumer/kafka_to_s3.py
        owner: "{{ kafka_user }}"
        group: "{{ kafka_user }}"
        mode: "0755"

    - name: Create consumer systemd service
      copy:
        dest: /etc/systemd/system/kafka-consumer.service
        content: |
          [Unit]
          Description=Kafka to S3 Consumer
          After=kafka.service redis.service
          Requires=kafka.service redis.service

          [Service]
          Type=simple
          User={{ kafka_user }}
          WorkingDirectory=/opt/consumer
          ExecStart=/usr/bin/python3 /opt/consumer/kafka_to_s3.py
          Restart=on-failure
          RestartSec=10

          [Install]
          WantedBy=multi-user.target
        mode: "0644"

    # ========================================
    # START SERVICES
    # ========================================

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start Kafka
      systemd:
        name: kafka
        enabled: yes
        state: started

    - name: Enable and start Redis
      systemd:
        name: redis-server
        enabled: yes
        state: started

    - name: Wait for Kafka to be ready
      wait_for:
        port: 9092
        delay: 5
        timeout: 60

    # ========================================
    # CREATE KAFKA TOPICS
    # ========================================

    - name: Create Kafka topics
      shell: |
        {{ kafka_home }}/bin/kafka-topics.sh --create \
          --bootstrap-server localhost:9092 \
          --topic {{ item }} \
          --partitions 3 \
          --replication-factor 1 \
          --if-not-exists
      loop:
        - bluesky
        - nostr
        - hackernews
        - stackoverflow
        - rss
      become_user: "{{ kafka_user }}"

    - name: List Kafka topics
      shell: "{{ kafka_home }}/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list"
      register: kafka_topics
      become_user: "{{ kafka_user }}"

    - name: Display created topics
      debug:
        msg: "Topics created: {{ kafka_topics.stdout_lines }}"

    # ========================================
    # START CONSUMER
    # ========================================

    - name: Enable and start consumer
      systemd:
        name: kafka-consumer
        enabled: yes
        state: started

  handlers:
    - name: restart redis
      systemd:
        name: redis-server
        state: restarted
